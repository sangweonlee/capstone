import streamlit as st
import os
from dotenv import load_dotenv
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
import google.generativeai as genai
from langchain.embeddings import HuggingFaceEmbeddings
import tempfile

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")
genai.configure(api_key=google_api_key)

# ì»¤ìŠ¤í…€ CSS ìŠ¤íƒ€ì¼ë§
def set_custom_style():
    st.markdown("""
    <style>
    .main {
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 10px;
    }
    .stTitle {
        color: #2c3e50;
        font-family: 'Arial', sans-serif;
        text-align: center;
        font-size: 2.5em;
        margin-bottom: 0;
    }
    .stMarkdown p {
        color: #34495e;
        font-size: 1.1em;
    }
    .stFileUploader {
        background-color: #ffffff;
        border: 2px dashed #3498db;
        border-radius: 10px;
        padding: 20px;
    }
    .stChatMessage.user {
        background-color: #3498db;
        color: white;
        border-radius: 15px;
        padding: 10px;
        margin: 5px 0;
    }
    .stChatMessage.assistant {
        background-color: #2ecc71;
        color: white;
        border-radius: 15px;
        padding: 10px;
        margin: 5px 0;
    }
    .stSpinner > div > div {
        border-color: #e74c3c transparent transparent transparent !important;
    }
    .stSuccess {
        background-color: #27ae60;
        color: white;
        border-radius: 5px;
        padding: 10px;
    }
    .stInfo {
        background-color: #3498db;
        color: white;
        border-radius: 5px;
        padding: 10px;
    }
    .stError {
        background-color: #e74c3c;
        color: white;
        border-radius: 5px;
        padding: 10px;
    }
    </style>
    """, unsafe_allow_html=True)

# 1. ì—…ë¡œë“œëœ ë‹¤ì¤‘ PDF ë¬¸ì„œ ë¡œë“œ
def load_pdfs_from_upload(uploaded_files):
    all_documents = []
    if uploaded_files:
        for uploaded_file in uploaded_files:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                tmp_file.write(uploaded_file.read())
                tmp_file_path = tmp_file.name
            loader = PyPDFLoader(tmp_file_path)
            documents = loader.load()
            all_documents.extend(documents)
            os.remove(tmp_file_path)
    return all_documents

# 2. í…ìŠ¤íŠ¸ ë¶„í• 
def split_documents(documents):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = text_splitter.split_documents(documents)
    return split_docs

# 3. ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
def create_vector_store(docs):
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
    vector_store = FAISS.from_documents(docs, embeddings)
    return vector_store

# 4. Geminië¥¼ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±
def get_gemini_response(question, context):
    model = genai.GenerativeModel('gemini-2.0-flash-001')
    prompt = f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.\në¬¸ì„œ ë‚´ìš©: {context}\nì§ˆë¬¸: {question}"
    response = model.generate_content(prompt)
    return response.text

# 5. ì±—ë´‡ ì„¤ì •
def setup_chatbot(vector_store):
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
    
    def chat_with_memory(question):
        docs = vector_store.similarity_search(question, k=2)
        context = "\n".join([doc.page_content for doc in docs])
        chat_history = memory.load_memory_variables({})["chat_history"]
        if chat_history:
            context += f"\nì´ì „ ëŒ€í™”: {chat_history[-2:]}"
        answer = get_gemini_response(question, context)
        memory.save_context({"question": question}, {"answer": answer})
        return answer
    return chat_with_memory

# Streamlit ì•±
def main():
    set_custom_style()  # ì»¤ìŠ¤í…€ ìŠ¤íƒ€ì¼ ì ìš©
    
    # í—¤ë”ì™€ ì„¤ëª…
    st.title("ğŸ“š ë‹¤ì¤‘ PDF ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ í€´ì¦ˆ ìƒì„± ì±—ë´‡!")
    st.markdown("""
    ì—¬ëŸ¬ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ë¬¸ì„œ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ í€´ì¦ˆë¥¼ í’€ì–´ë³´ì„¸ìš”!  
    AIê°€ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ê³  ì‹¤ì‹œê°„ìœ¼ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.
    """, unsafe_allow_html=True)



    # PDF ì—…ë¡œë“œ ìœ„ì ¯
    uploaded_files = st.file_uploader(
        "ğŸ“„ PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
        type=["pdf"],
        accept_multiple_files=True,
        help="ì—¬ëŸ¬ PDF íŒŒì¼ì„ ì„ íƒí•˜ì—¬ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )

    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "chatbot" not in st.session_state:
        st.session_state.chatbot = None

    # ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬
    if uploaded_files and st.session_state.chatbot is None:
        with st.spinner("PDFë“¤ì„ ë¶„ì„ ì¤‘... â³"):
            documents = load_pdfs_from_upload(uploaded_files)
            if documents:
                split_docs = split_documents(documents)
                vector_store = create_vector_store(split_docs)
                st.session_state.chatbot = setup_chatbot(vector_store)
                st.success(f"âœ… {len(uploaded_files)}ê°œì˜ PDF ë¶„ì„ ì™„ë£Œ! ì±—ë´‡ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                st.error("âŒ PDF ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")

    # ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤
    if st.session_state.chatbot:
        # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # ëŒ€í™” ì»¨í…Œì´ë„ˆ
        chat_container = st.container()
        with chat_container:
            for message in st.session_state.messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])

        # ì‚¬ìš©ì ì…ë ¥
        if question := st.chat_input("í€´ì¦ˆ ì£¼ì œì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: 'ëœë¤ í€´ì¦ˆë¥¼ ë§Œë“¤ì–´ì¤˜')"):
            st.session_state.messages.append({"role": "user", "content": question})
            with chat_container:
                with st.chat_message("user"):
                    st.markdown(question)
                with st.chat_message("assistant"):
                    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘... ğŸ¤–"):
                        answer = st.session_state.chatbot(question)
                        st.markdown(answer)
                    st.session_state.messages.append({"role": "assistant", "content": answer})
    else:
        st.info("PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ì±—ë´‡ì´ í™œì„±í™”ë©ë‹ˆë‹¤. ğŸš€")

if __name__ == "__main__":
    main()
